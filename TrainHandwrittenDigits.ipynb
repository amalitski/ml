{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML2s.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOvhaF15FH+gLtsjNRDwuNH"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2rZkmn-aoc_",
        "outputId": "bee495e7-a1b3-4601-80d7-1ec98fe0c61f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "n_epochs = 3\n",
        "batch_size_train = 64\n",
        "batch_size_test = 1000\n",
        "log_interval = 10\n",
        "\n",
        "random_seed = 2022\n",
        "torch.backends.cudnn.enabled = False\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "#GPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device: %s\" % device)\n",
        "\n",
        "#60k handwritten digits for train\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('/data/', train=True, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
        "                             ])),\n",
        "  batch_size=batch_size_train, shuffle=True)\n",
        "#10k for test\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('/data/', train=False, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
        "                             ])),\n",
        "  batch_size=batch_size_test, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHFqN6ABwq_A",
        "outputId": "1415d227-979c-4b69-e512-0582d32efe8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#network\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x)\n",
        "\n",
        "\n",
        "network = Net()\n",
        "optimizer = optim.SGD(network.parameters(), lr=0.01, momentum=0.5)\n"
      ],
      "metadata": {
        "id": "ttne9eqUxHul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train & test\n",
        "train_losses = []\n",
        "train_counter = []\n",
        "test_losses = []\n",
        "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
        "\n",
        "def train(epoch):\n",
        "  network.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    optimizer.zero_grad()\n",
        "    output = network(data)\n",
        "    loss = F.nll_loss(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx % log_interval == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "        100. * batch_idx / len(train_loader), loss.item()))\n",
        "      train_losses.append(loss.item())\n",
        "      train_counter.append(\n",
        "        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
        "      torch.save(network.state_dict(), '/tmp/model.pth')\n",
        "      torch.save(optimizer.state_dict(), '/tmp/optimizer.pth')\n",
        "\n",
        "\n",
        "def test():\n",
        "  network.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      output = network(data)\n",
        "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
        "      pred = output.data.max(1, keepdim=True)[1]\n",
        "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  test_losses.append(test_loss)\n",
        "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "    test_loss, correct, len(test_loader.dataset),\n",
        "    100. * correct / len(test_loader.dataset)))\n",
        "  \n",
        "\n",
        "import os\n",
        "if os.path.exists(\"/tmp/model.pth\"):\n",
        "  os.remove(\"/tmp/model.pth\")\n",
        "  os.remove(\"/tmp/optimizer.pth\")\n",
        "  print(\"Previous model has been\")\n",
        "\n",
        "test()\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  train(epoch)\n",
        "  test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBLgxkYjxhul",
        "outputId": "efc8ad03-f962-45dc-c283-b79d523333ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Previous model has been\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Avg. loss: 2.3050, Accuracy: 858/10000 (9%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.347104\n",
            "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.316271\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.305171\n",
            "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.283131\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.271820\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.259622\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.192772\n",
            "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.150468\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.147990\n",
            "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.131485\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.024535\n",
            "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 1.859812\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.900070\n",
            "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 1.788876\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.574270\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.541650\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.424905\n",
            "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 1.021533\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 1.273104\n",
            "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 1.133314\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.149860\n",
            "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 1.017837\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.904310\n",
            "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 1.131544\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.922589\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 1.145692\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.983944\n",
            "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.780443\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.937160\n",
            "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.746564\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.747251\n",
            "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.732935\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 1.001296\n",
            "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 1.010068\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.637782\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.608854\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.967186\n",
            "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.906461\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.601401\n",
            "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.800752\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.592654\n",
            "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.935263\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.602909\n",
            "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.674705\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.694951\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.767586\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.722246\n",
            "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.750190\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.417136\n",
            "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.815795\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.572984\n",
            "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.655913\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.758314\n",
            "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.691017\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.573594\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.497146\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.776160\n",
            "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.799221\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.588472\n",
            "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.609805\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.674256\n",
            "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.380433\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.438626\n",
            "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.965664\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.554563\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.786838\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.631809\n",
            "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.624102\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.814030\n",
            "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.613699\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.643000\n",
            "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.519498\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.271380\n",
            "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.304119\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.391136\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.583594\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.521231\n",
            "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.515231\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.464233\n",
            "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.734536\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.530428\n",
            "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.472211\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.421755\n",
            "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.430562\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.419645\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.428863\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.456193\n",
            "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.315919\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.235505\n",
            "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.496714\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.310535\n",
            "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.588385\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.361635\n",
            "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.394569\n",
            "\n",
            "Test set: Avg. loss: 0.1946, Accuracy: 9422/10000 (94%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.371104\n",
            "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.459858\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.616352\n",
            "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.340721\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.439978\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.338264\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.523775\n",
            "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.523187\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.295559\n",
            "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.542221\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.478928\n",
            "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.416450\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.579413\n",
            "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.355888\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.485590\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.388809\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.450187\n",
            "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.724044\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.387525\n",
            "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.289531\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.670373\n",
            "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.576281\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.433720\n",
            "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.310877\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.518332\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.409285\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.363400\n",
            "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.564027\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.436106\n",
            "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.306033\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.701799\n",
            "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.598650\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.395665\n",
            "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.486890\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.445360\n",
            "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.103259\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.352660\n",
            "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.503096\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.491115\n",
            "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.471447\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.263011\n",
            "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.570634\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.301996\n",
            "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.317493\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.241467\n",
            "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.268655\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.524569\n",
            "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.472901\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.352816\n",
            "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.375248\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.671598\n",
            "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.398484\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.374732\n",
            "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.391395\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.411615\n",
            "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.414984\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.330763\n",
            "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.308291\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.279973\n",
            "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.493614\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.244288\n",
            "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.390043\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.329448\n",
            "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.245541\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.506786\n",
            "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.540775\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.675545\n",
            "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.250624\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.153791\n",
            "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.273074\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.344255\n",
            "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.365249\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.177741\n",
            "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.280459\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.305034\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.312350\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.590578\n",
            "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.397274\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.347686\n",
            "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.207186\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.235654\n",
            "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.192118\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.378828\n",
            "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.317931\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.291649\n",
            "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.612163\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.273604\n",
            "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.297077\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.432684\n",
            "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.315957\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.243331\n",
            "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.307325\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.340667\n",
            "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.193930\n",
            "\n",
            "Test set: Avg. loss: 0.1250, Accuracy: 9619/10000 (96%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.237738\n",
            "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.438878\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.300183\n",
            "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.316669\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.188601\n",
            "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.238796\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.276435\n",
            "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.295632\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.371547\n",
            "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.427480\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.248754\n",
            "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.482954\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.546523\n",
            "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.273461\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.387594\n",
            "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.346154\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.437009\n",
            "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.264583\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.368365\n",
            "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.149694\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.211977\n",
            "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.467497\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.205825\n",
            "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.235903\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.575893\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.237814\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.271453\n",
            "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.302163\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.331400\n",
            "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.198665\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.267812\n",
            "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.248105\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.496887\n",
            "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.118220\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.306055\n",
            "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.204110\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.159477\n",
            "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.488728\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.252238\n",
            "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.368657\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.267396\n",
            "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.281516\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.181112\n",
            "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.455665\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.210577\n",
            "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.392216\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.302841\n",
            "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.250734\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.239293\n",
            "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.369299\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.175470\n",
            "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.352025\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.303633\n",
            "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.327489\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.372362\n",
            "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.171098\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.221412\n",
            "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.310344\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.288104\n",
            "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.389881\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.183542\n",
            "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.294355\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.262859\n",
            "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.269880\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.112218\n",
            "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.199911\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.335500\n",
            "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.158232\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.166302\n",
            "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.395647\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.289369\n",
            "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.320623\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.128569\n",
            "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.240963\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.187338\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.362554\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.182405\n",
            "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.234739\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.171027\n",
            "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.332755\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.428166\n",
            "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.272035\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.432870\n",
            "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.291809\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.281410\n",
            "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.228769\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.364152\n",
            "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.379311\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.183557\n",
            "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.340983\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.294742\n",
            "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.341900\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.382918\n",
            "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.323704\n",
            "\n",
            "Test set: Avg. loss: 0.1014, Accuracy: 9698/10000 (97%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction\n",
        "import matplotlib.pyplot as plt\n",
        "examples = enumerate(test_loader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)\n",
        "\n",
        "with torch.no_grad():\n",
        "  output = network(example_data)\n",
        "\n",
        "fig = plt.figure(figsize=(10,2))\n",
        "fig.suptitle('prediction/target', fontsize=20)\n",
        "for i in range(10):\n",
        "  fig.add_subplot(1,10,i+1)\n",
        "  plt.tight_layout()\n",
        "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
        "  plt.title(\"{} / {}\".format(output.data.max(1, keepdim=True)[1][i].item(), example_targets[i]))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "id": "-WF2b3yOyLmZ",
        "outputId": "b05b06dc-a470-43c9-abdf-1f65fb1c485c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x144 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YKDHL7wHiLhc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}